DR-RD Repository Audit – 2025-09-12
1.	Pipeline Conformance (Score: 9/10)
Unified Pipeline Execution: The system adheres closely to the Playbook’s standardized pipeline: Planner → Router → Executor → Synthesizer[1]. The Planner produces a structured task list in JSON (validated against a schema) which is then normalized and stored[2][3]. A Router uses a centralized AGENT_REGISTRY to map each task’s role to the appropriate agent class[4][5]. The Orchestrator sequentially (with parallelism enabled for efficiency) invokes each agent and collects their outputs before running a final synthesis stage[6]. This matches the addendum’s “standardized pipeline” design, with each stage clearly delineated.
Strict JSON Contracts: JSON-only outputs are enforced at every stage, reflecting the Playbook’s reliability hardening goals[7]. The Planner and agents are instructed to output only JSON, and the Orchestrator validates and parses their responses. The code uses a validate_and_retry mechanism to automatically detect malformed JSON or placeholder content and re-invoke the agent with a tightened prompt[8]. For example, logs from the 2025-09-12 run show retry_prompt events where an agent’s first response was corrected with a follow-up prompt explicitly demanding valid JSON【60†lines 2-10】【60†lines 11-19】. In cases of repeated failure or missing data, the Orchestrator inserts safe default values (“Not determined”) to ensure downstream steps receive a well-formed JSON object[9][10]. This means the pipeline never breaks on format errors – if an agent’s output is empty or invalid after retries, the system logs an open issue and continues with a placeholder result[11][12]. These measures align with Playbook guidance on robust JSON-only processing and were effective in the latest run (every task ultimately yielded a JSON result for synthesis).
2.	Architectural Gaps (Score: 8/10)
Orchestrator & Registry Alignment: The implemented architecture largely matches the intended design. The unified agent registry is in place (mapping roles like CTO, Regulatory, QA, etc. to their agent classes) and the router logic uses it to instantiate the correct agent for each task[4][13]. Business logic resides in orchestrator modules (core/orchestrator.py, executor, etc.), keeping the Streamlit UI layer thin as recommended. The architecture covers planning, execution, and synthesis stages without any major component missing. The tasks from the Planner in the 09-12 run (Materials, Regulatory, Design/CTO, Finance, Marketing, QA) were all successfully routed to their respective agents, confirming the registry and router are functioning as expected.
Final Synthesizer Invocation: One minor divergence remains in the final synthesis stage. Instead of fully delegating to the SynthesizerAgent to generate the final output strictly via its JSON schema, the Orchestrator currently aggregates all agent outputs and calls the LLM to compose the final report, then parses that result[14][15]. The code does use the Synthesizer’s schema (synthesizer_v1.json) in the prompt and attempts to json.loads the LLM’s output[14], but this last step isn’t as rigorously constrained – if the final output isn’t valid JSON, the system simply wraps it into a default format[16] rather than retrying the way earlier stages do. In the latest run, the final combined output was coherent and complete, but we observed a slight format anomaly (discussed below) that likely stems from this approach. Ensuring the final report is generated via a strictly schema-valid JSON (and then rendered to Markdown) is an improvement area (addressed in the plan). Functionally, the system produces a merged report, so this gap is more about consistency and robustness than missing functionality.
Agent Roster and Reachability: The agent roster in code goes beyond the initial 5–8 core roles, in line with Playbook guidance to expand specialists. All major roles (Planner, CTO, Research Scientist, Materials Engineer, Regulatory, IP Analyst, QA, Finance, etc.) are present, and additional ones like Chief Scientist, Mechanical Systems Lead, and a Reflection (reviewer) agent are defined[4][13]. The presence of these extra roles is positive, but as noted previously, we must ensure they are all actually usable. In the 09-12 run, none of the tasks required the Mechanical Systems Lead or Chief Scientist, and indeed those agents were not invoked. The router’s keyword mapping has been updated to include “mechanical” triggers[17], which partially addresses the previous audit’s concern that MechanicalSystemsLeadAgent was unreachable. However, a full sweep shows a few roles might still be “orphaned” (defined in code but not triggered by any Planner output or alias). For example, if the Planner never outputs a task for a Finance Specialist vs. Finance, or if both exist interchangeably, one could be redundant. This is a minor integration gap – essentially cleanup of legacy or duplicate roles and confirming every agent in the registry has a corresponding route. Plans are underway (per dev discussions) to audit and consolidate such roles, which will close this gap[18][19]. Overall, the architecture is sound; remaining gaps are about consistency and eliminating dead code rather than structural flaws.
3.	Prompt and Contract Conformance (Score: 9/10)
Standardized Prompt Templates: All agents follow a unified prompt schema and contract. In code, each agent subclass of PromptFactoryAgent constructs a spec with the same fields – including its role name, a task description, an io_schema_ref pointing to the JSON schema for its output, and often an evaluation_hooks list for self-checks[20][21]. This means the CTOAgent, MaterialsEngineerAgent, RegulatoryAgent, FinanceAgent, etc., all use a consistent format for prompts and outputs. The Playbook’s mandate for consistent prompt contracts is clearly met: for example, the MarketingAgent defines "role": "Marketing Analyst" and uses the marketing_v2.json schema[22], the FinanceAgent similarly uses a finance_v1.json schema (not shown here but present in the code), ensuring each knows exactly which fields (findings, risks, next_steps, sources, etc.) to produce. The PlannerAgent likewise produces a JSON list of tasks with defined fields (id, title, description, role, etc.), and the system normalizes any missing fields so that agents always receive the expected structure[3][23]. This uniformity was evident in the 09-12 run – each agent output JSON contained the standard keys (summary, findings, risks, next_steps, sources, etc.), with no agent deviating from the prescribed format.
JSON Output Formatting: The implementation takes extra steps to enforce well-formatted JSON outputs. After an agent returns text, the Orchestrator extracts the JSON block (ignoring any extraneous content)[24][25] and sanitizes known issues (for instance, it removes a combined_context key if present, and normalizes placeholder values). The code accounts for common LLM quirks like adding markdown fences or non-JSON commentary: utility functions handle extracting JSON from a mixture of text, and the prompt instructions themselves forbid markdown or extra keys (as seen in the retry prompts in logs, which explicitly warn “Do not use markdown formatting in any JSON field… no other keys may be added”【60†lines 13-19】【60†lines 20-28】). Additionally, the schemas enforce types (e.g. risks and next_steps must be arrays; sources must be a list of strings or objects depending on agent). The system will coerce types if needed and drop any illegal entries (e.g. empty objects in sources). The result is that agent outputs are clean. In the latest run, for example, all agents produced JSON without stray markdown or missing fields. We also see continuous improvements: previously, some agents might have returned a single string for a list field if only one item, but now schema v2 updates ensure even single entries are wrapped in an array, maintaining consistency. The only formatting lapse observed was at the final synthesis stage (as noted above, not fully JSON-constrained) – which led to a minor output quirk, but not a contract violation by an agent.
Contract Enforcement and Self-Checks: The project treats prompts as “versioned code” and uses self-check hooks to validate outputs. Many agents include evaluation functions that run on their result before finalizing. For instance, the IPAnalystAgent has a hook to check for patent overlap (ensuring the findings include patent references), and others have minimal self_check hooks to verify JSON validity[26][27]. The Orchestrator integrates these checks via invoke_agent_safely and the validate_and_retry loop: if an output fails validation (e.g. JSON invalid or content placeholders detected), the agent is re-prompted automatically[8][28]. This was confirmed in the run logs – e.g., the CTO agent’s first output didn’t conform to schema, triggering an automatic second call with a stricter prompt (which succeeded). Thanks to these mechanisms, we did not encounter any agent output in this run that broke the JSON contract or contained disallowed content. Every task either passed validation or was retried until it did. The one-point deduction here remains due to the Synthesizer stage being slightly outside the strict schema enforcement (no automatic schema retry on the final LLM call). Once that is addressed (as planned in task T1 of the codex plan), the prompt conformance would be a perfect 10.
4.	Artifact Completeness and Output Quality (Score: 9/10)
Repository Artifacts: All expected code and config artifacts from the Playbook are present in the repo. The structured layout — with directories for agents/, core/ logic, dr_rd/ domain-specific modules, prompts/ for prompt templates, config/ for settings, and the Streamlit app.py front-end — corresponds exactly to the Playbook’s recommended scaffold[29][30]. Important schemas (e.g. dr_rd/schemas/*.json for each agent role) are versioned and included, and tools/integrations (search, simulation, etc.) have their configurations in config/tools.yaml and related files. A full suite of tests exists under tests/, covering unit tests for core orchestration, agent outputs, redaction, etc. The CI configuration ensures these tests run along with linting and even checks for secrets, reflecting a mature development process. In terms of run artifacts from the 2025-09-12 execution, all expected outputs were generated: we have the planned task list (plan.json), each agent’s result (visible in the combined trace), and the final synthesized report (report.md), as well as logs and an api_calls.csv with usage stats. Every planned task produced an output – there were 6 tasks planned and 6 agent outputs, which then fed into the final report – indicating no task was dropped or left unaddressed.
Output Formatting Issues: The final user-facing artifacts are comprehensive, but there are minor formatting issues to fix, which is why this category is marked slightly down from perfect. Notably, the Overview section of the final report included the “Idea” in an internal representation format (e.g. it showed Idea: ('a quantum entanglement microscope', {}, set()) instead of a clean description). This tuple contains the idea string, an empty constraints dict, and an empty set of toggles, directly reflecting the internal intake data structure. This is a cosmetic bug – the report should display just the idea (and possibly note “no constraints” rather than {}) for clarity. Another issue is in the Trace summary table of the report: many fields were “None” or blank for the intermediate steps, and the step names for the final Synthesizer phase were not shown. For example, steps 1–6 in the table had None under the name/status columns despite those tasks actually executing, and the final step (step 7) was listed as “synth” but with no descriptive name. This appears to be a glitch in how the summary data is compiled or rendered (likely the build_rows in traceability or the Streamlit session state not populating certain fields). Additionally, the Metrics section reported 0 tokens used and $0.0000 cost, which is clearly not accurate given multiple GPT-4 calls were made. It seems the integration with the CostTracker (which tracks token usage/cost from OpenAI API responses) did not update the report. These are relatively minor presentation issues – the underlying data exists (the run’s api_calls.csv shows the actual token counts per call), but the final markdown isn’t pulling it in. Ensuring the final report rehydrates and formats all fields correctly (idea, step names, token counts, etc.) will make the artifacts truly complete and polished. Aside from these polish items, the artifacts cover everything they should: each agent’s findings, risks, and next steps are present either in the detailed Trace or rolled into the final report, and sources from research agents (if any) would appear in the final “Sources” section (none were needed for this particular idea, which is why that section is empty). All logs and intermediate files were captured. In summary, the run produced a full set of outputs for the given input, with no missing pieces – the slight score reduction is only for the formatting inconsistencies in the report/trace display, which are being addressed.
5.	Redaction and Privacy Handling (Score: 10/10)
Input Redaction & Context Isolation: The system continues to excel at privacy protection, fully adhering to the Playbook’s guidance to “obfuscate each subtask” and provide agents only the minimum necessary context[31][32]. Before any user input or task description is sent to a model or tool, it passes through the redaction layer. In the code, the Orchestrator’s _invoke_agent uses a Redactor to replace sensitive tokens (proper names, organizations, specific product names) with generic placeholders[33][34]. In the 09-12 run, the main idea “a quantum entanglement microscope” isn’t particularly sensitive (it’s a concept, not a personal info), but the system still treated it as a project name and converted it to a placeholder internally (QuantumEntanglementMicroscopeDevice) to avoid leaking potentially identifying context. Each agent, therefore, saw a sanitized version of the idea in its prompt (verified in the logs, where Input preview (redacted) shows the idea replaced with that token【50†lines 1-4】【50†lines 5-12】). Agents only received information relevant to their task: e.g. the Materials agent got the idea alias and a task description about material selection, without being exposed to, say, financial or marketing details. This need-to-know scoping is in place across the board.
No Privacy Leaks in Outputs: During processing, all intermediate outputs used the pseudonyms (the trace artifacts show the placeholder name rather than the real idea in agent outputs), and crucially, the system rehydrates the placeholders back to the original terms for the final user-facing report. The final report given to the user refers to “quantum entanglement microscope” in natural language, not the Device alias, thanks to a rehydration step that replaces the placeholders using the stored alias map[35][36]. This ensures the user sees a meaningful result while all internal handling protected the raw input. We saw no instances of unredacted sensitive info appearing in logs or intermediate artifacts. Even the debug logs prefix entries with “(redacted)” and confirm that keys (like API keys) are not printed. Furthermore, the repository contains no hard-coded secrets – all API keys (OpenAI, SerpAPI, etc.) are supplied via environment variables or Streamlit secrets, consistent with best practices and Playbook rules (no secrets in code)[37][38]. An .env.example file is provided to guide configuration, and secret scanning is part of CI, ensuring nothing slips in[39][40].
Data Retention and Rehydration: The approach of using placeholders and then rehydrating means that even if logs or agent outputs were exposed, they contain no real personal data – only generic stand-ins. After the run, the final artifacts (like the report) have been cleansed of placeholders. For example, the combined findings in the final report don’t contain terms like “QuantumEntanglementMicroscopeDevice” at all – that alias was entirely an internal device. This design shows a strong compliance with privacy-by-design. We also note that the system retains an alias_map for potential further use (perhaps for follow-up queries or future multi-run context), but it does not leak that map in any user-visible output. Overall, the privacy and redaction mechanisms are operating at a high standard. The audit did not find any privacy or PII leakage issues in the 2025-09-12 run. By obfuscating at input and rehydrating at output, DR-RD achieves a seamless balance between using real context internally and protecting sensitive information, fully meeting the Playbook’s privacy goals.
Sources: This audit is based on the latest DR-RD codebase (clcriswell/DR-RD on GitHub) and the “DRRD Playbook — Consolidated Edition Addendum” (Aug 20, 2025)[41][31]. Specific file references and log excerpts are included inline above. The 2025-09-12 system run logs and artifacts were analyzed to verify behavior against the Playbook’s objectives. Overall, the DR-RD implementation is in strong alignment with the intended architecture and principles, with only minor issues in final output formatting and some cleanup tasks remaining – all of which are noted for remediation in the accompanying plan.
