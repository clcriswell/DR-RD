tasks:
- id: T6  
  group: report-polish  
  description: "Polish the final report and trace output formatting to fix placeholder and null values (ensure the idea, task names, and metrics are presented clearly to the user)."  
  acceptance_criteria:
    - "The **Idea** in the report overview is displayed in a clean, user-friendly format. If no constraints or toggles were provided, it should simply show the idea (and possibly a note like 'Constraints: None') instead of a raw Python tuple/dict. The internal representation `('idea', {}, set())` should be eliminated from user-facing output."  
    - "The **Trace summary table** in the report is fully populated with real data. Each step’s `phase` and `name` columns show the proper labels (e.g. 'executor' phase with the task name like 'Materials Selection for Quantum Components', and 'synthesizer' phase with 'Final Synthesis' or similar). No cell should display `None` or remain blank if information is available."  
    - "All metrics in the report (steps count, token usage, cost) reflect actual values. If the run consumed tokens/cost, those numbers are shown instead of 0. If certain metrics are not applicable, the report should omit them or explicitly state 'N/A' rather than showing incorrect zeros. The linkage between the CostTracker (or `api_calls`) and the report is fixed so that token and cost totals update correctly."  
    - "No placeholder aliases (like 'QuantumEntanglementMicroscopeDevice') or internal debug strings appear anywhere in the final Markdown report or trace. All such placeholders must be rehydrated or removed. Likewise, any sections meant to list content (e.g. sources, safety notes, etc.) should either display the relevant entries or be omitted if empty – without leaving section headers with empty content."  
  test_coverage: "Add/extend a unit test for the report generation. Simulate a run by populating `st.session_state` with a sample intake (idea, constraints, toggles) and a set of fake task results, including a dummy CostTracker with non-zero token counts. Invoke the final report composition (e.g. via `core.orchestrator` or `core.final.composer.write_final_bundle`) and parse the output Markdown. Assert that the 'Idea' line contains only the input idea text (and any constraints in readable form), that each planned task name appears in the trace summary table, and that token/cost metrics match the dummy data provided. The test should fail if any `None` or placeholder patterns are present in the output. Additionally, verify through this test (or a separate one) that when cost tracking is enabled and records exist (e.g. via a fake OpenAI response log), the reported token count and cost are non-zero in the output."  
  labels: ["output-format", "bug", "usability"]  




